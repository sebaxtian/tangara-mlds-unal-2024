{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.006004Z",
     "start_time": "2024-09-10T15:21:32.516526Z"
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geohash2\n",
    "\n",
    "from io import StringIO\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from requests import Response\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Explicitly providing path to '.env'\n",
    "from pathlib import Path  # Python 3.6+ only\n",
    "# Load .env variables\n",
    "_ = load_dotenv(dotenv_path=f\"{Path().resolve().parents[0]}/.env\")\n",
    "\n",
    "# Logger\n",
    "from loguru import logger\n",
    "logger.add(f\"{Path().resolve().parents[0]}/logs/tangara.log\", rotation=\"1 MB\", retention=\"30 days\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.023462Z",
     "start_time": "2024-09-10T15:21:33.018271Z"
    }
   },
   "source": [
    "def to_timestamp(datetime_iso8601: str) -> int:\n",
    "    \"\"\"\n",
    "    Datetime ISO 8601 Format to Timestamp\n",
    "    TZ='America/Bogota' -05:00\n",
    "\n",
    "    :params\n",
    "    :datetime_iso8601: str, Datetime ISO 8601 Format\n",
    "\n",
    "    :return: int, Timestamp\n",
    "\n",
    "    :example\n",
    "        - to_timestamp('2023-03-17T00:00:00-05:00')\n",
    "            return: 1679029200000\n",
    "    \"\"\"\n",
    "    timestamp = int(datetime.fromisoformat(datetime_iso8601).timestamp() * 1000)\n",
    "    \n",
    "    logger.debug(\"Run to_timestamp:\")\n",
    "    logger.debug(f\"datetime_iso8601: {datetime_iso8601}, Timestamp: {timestamp}\")\n",
    "    \n",
    "    return timestamp"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.176189Z",
     "start_time": "2024-09-10T15:21:33.172260Z"
    }
   },
   "source": [
    "def request_influxdb(sql_query: str) -> Response:\n",
    "    \"\"\"\n",
    "    Request to InfluxDB API REST\n",
    "\n",
    "    :params\n",
    "    :sql_query: str, InfluxDB SQL query\n",
    "\n",
    "    :return: Response, InfluxDB response as CSV text\n",
    "    \"\"\"\n",
    "    endpoint = os.getenv(\"URL_INFLUXDB_QUERY_ENDPOINT\", None)\n",
    "    database = os.getenv(\"DB_NAME_INFLUXDB\", None)\n",
    "    parameters = {\n",
    "        'db': database,\n",
    "        'q': sql_query,\n",
    "        'epoch': 'ms',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    # To get response as CSV text\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    # GET Request\n",
    "    response = requests.get(endpoint, params=parameters, headers=headers)\n",
    "    \n",
    "    logger.debug(\"Run request_influxdb:\")\n",
    "    logger.debug(f\"response: {response}\")\n",
    "    \n",
    "    return response"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.217338Z",
     "start_time": "2024-09-10T15:21:33.213768Z"
    }
   },
   "source": [
    "def query_tangaras(start_timestamp: int, end_timestamp: int) -> str:\n",
    "    \"\"\"\n",
    "    Get InfluxDB SQL query of all Tangara sensors that have reported data over a period of time.\n",
    "\n",
    "    :params:\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "\n",
    "    :return: str, InfluxDB SQL Query\n",
    "    \"\"\"\n",
    "    # Period DateTime\n",
    "    period_time = f\"time >= {start_timestamp}ms AND time <= {end_timestamp}ms\"\n",
    "    # SQL\n",
    "    sql_query = \"SELECT DISTINCT(geo) AS \\\"geohash\\\" \"\\\n",
    "                \"FROM \\\"fixed_stations_01\\\" WHERE \"\\\n",
    "                \"(\\\"geo3\\\" = 'd29') AND \"\\\n",
    "                f\"{period_time} \"\\\n",
    "                \"GROUP BY \\\"name\\\";\"\n",
    "    \n",
    "    logger.debug(\"Run query_tangaras:\")\n",
    "    logger.debug(f\"sql_query: {sql_query}\")\n",
    "    \n",
    "    return sql_query"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.265402Z",
     "start_time": "2024-09-10T15:21:33.261042Z"
    }
   },
   "source": [
    "def query_measure(mac_tangaras: [str], start_timestamp: int, end_timestamp: int, measure: str='pm25', group_by_time: str='30s') -> str:\n",
    "    \"\"\"\n",
    "    Get InfluxDB SQL query for specific measure (datatype) and for each Tangara sensor identified by MAC address between a period of time.\n",
    "\n",
    "    :params:\n",
    "    :mac_tangaras: [str], Tangara sensor MAC address\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "    :measure: str, choice ['pm25', 'tmp', 'hum']\n",
    "    :group_by_time: str, choice ['30s', '1m', '1h']\n",
    "\n",
    "    :return: str, InfluxDB SQL Query\n",
    "    \"\"\"\n",
    "    # Period DateTime\n",
    "    period_time = f\"time >= {start_timestamp}ms AND time <= {end_timestamp}ms\"\n",
    "    # SQL Datatype by Tangara Sensor\n",
    "    sql_query = \"\"\n",
    "    head = \"last(\" if group_by_time == '30s' else \"mean(\"\n",
    "    for mac in mac_tangaras:\n",
    "        sql_query += f\"SELECT {head}\\\"{measure}\\\") \"\\\n",
    "                    \"FROM \\\"fixed_stations_01\\\" WHERE \"\\\n",
    "                    f\"(\\\"name\\\" = '{mac}') AND \"\\\n",
    "                    f\"{period_time} \" \\\n",
    "                    f\"GROUP BY time({group_by_time}) fill(null); \"\n",
    "    sql_query = sql_query[:-2]\n",
    "    \n",
    "    logger.debug(\"Run query_measure:\")\n",
    "    logger.debug(f\"sql_query: {sql_query}\")\n",
    "    \n",
    "    return sql_query"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.311820Z",
     "start_time": "2024-09-10T15:21:33.308326Z"
    }
   },
   "source": [
    "def df_to_csv(df: DataFrame, filename: str, datafolder: str='0_raw') -> None:\n",
    "    \"\"\"\n",
    "    Save DataFrame into data folder as a CSV file.\n",
    "    datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "\n",
    "    :params:\n",
    "    :df: DataFrame, pandas DataFrame\n",
    "    :filename: str, CSV file name with extension .csv\n",
    "    :datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "    \"\"\"\n",
    "    # Save DataFrame into CSV file\n",
    "    path_datafolder=f\"{Path().resolve().parents[0]}/data/{datafolder}\"\n",
    "    df.to_csv(f\"{path_datafolder}/{filename}\")\n",
    "    \n",
    "    logger.debug(\"Run df_to_csv:\")\n",
    "    logger.debug(f\"Save DataFrame: {path_datafolder}/{filename}\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.360256Z",
     "start_time": "2024-09-10T15:21:33.355756Z"
    }
   },
   "source": [
    "def df_from_csv(filename: str, datafolder: str='0_raw', dtindex: bool=True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load DataFrame from CSV file localted in data folder.\n",
    "    datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "\n",
    "    :params:\n",
    "    :filename: str, CSV file name with extension .csv\n",
    "    :datafolder: str, choice ['0_raw', '1_clean', '2_features', 'backup']\n",
    "    :dtindex: bool, default True, Does the CSV file include and DATETIME column?\n",
    "    \n",
    "    :return: df: DataFrame, pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Load DataFrame from CSV file\n",
    "    path_csvfile=f\"{Path().resolve().parents[0]}/data/{datafolder}/{filename}\"\n",
    "    df_data = pd.read_csv(path_csvfile)\n",
    "    if dtindex:\n",
    "        df_data = df_data.set_index('DATETIME')\n",
    "        df_data.index = pd.to_datetime(df_data.index)\n",
    "        df_data: DataFrame = df_data.tz_convert(\"America/Bogota\")\n",
    "    \n",
    "    logger.debug(\"Run df_from_csv:\")\n",
    "    logger.debug(f\"Load DataFrame: {path_csvfile}\")\n",
    "    \n",
    "    return df_data"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.411265Z",
     "start_time": "2024-09-10T15:21:33.403438Z"
    }
   },
   "source": [
    "def df_tangara_sensors(start_timestamp: int, end_timestamp: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get Data Frame Tangaras Sensors of all Tangara sensors that have reported data over a period of time.\n",
    "\n",
    "    :params:\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "\n",
    "    :return: DataFrame, Tangaras Sensors\n",
    "    \"\"\"\n",
    "    # Query Tangaras\n",
    "    influxdb_sql_query_tangaras = query_tangaras(start_timestamp, end_timestamp)\n",
    "    # print(\"influxdb_sql_query_tangaras:\", influxdb_sql_query_tangaras)\n",
    "    # InfluxDB API REST Request\n",
    "    influxdb_request = request_influxdb(influxdb_sql_query_tangaras)\n",
    "    # print(\"influxdb_request:\", influxdb_request)\n",
    "    # print(\"influxdb_request.text:\", influxdb_request.text)\n",
    "\n",
    "    # Data Frame Tangaras\n",
    "    df_tangaras = pd.DataFrame([], columns=['ID','GEOHASH','MAC','GEOLOCATION','LATITUDE','LONGITUDE'])\n",
    "\n",
    "    # For each time series\n",
    "    for result in influxdb_request.json()['results']:\n",
    "        # print(\"result:\", result)\n",
    "        if 'series' in result:\n",
    "            for serie in result['series']:\n",
    "                # Get the serie\n",
    "                # print(\"serie:\", serie)\n",
    "\n",
    "                # DataFrame by statement_id and df_tangaras['ID']\n",
    "                tags = serie[\"tags\"]\n",
    "                columns = serie[\"columns\"]\n",
    "                values = serie[\"values\"]\n",
    "                # print(\"tags:\", tags)\n",
    "                # print(\"columns:\", columns)\n",
    "                # print(\"values:\", values)\n",
    "                df_tangara = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "                # Remove/Add Columns\n",
    "                df_tangara['MAC'] = tags['name']\n",
    "                df_tangara['GEOLOCATION'] = df_tangara['geohash'].apply(lambda x: \" \".join(str(value) for value in list(geohash2.decode_exactly(x)[0:2])))\n",
    "                df_tangara['LATITUDE'] = df_tangara['GEOLOCATION'].apply(lambda x: x.split(' ')[0])\n",
    "                df_tangara['LONGITUDE'] = df_tangara['GEOLOCATION'].apply(lambda x: x.split(' ')[1])\n",
    "                df_tangara['ID'] = df_tangara['MAC'].apply(lambda x: f\"TANGARA_{x[-4:]}\")\n",
    "                df_tangara.rename(columns={'geohash': 'GEOHASH'}, inplace=True)\n",
    "                df_tangara = df_tangara[['ID','GEOHASH','MAC','GEOLOCATION','LATITUDE','LONGITUDE']]\n",
    "\n",
    "                # print('df_tangara.head():', df_tangara.head())\n",
    "\n",
    "                df_tangaras = pd.concat([df_tangaras, df_tangara], ignore_index=True)\n",
    "    \n",
    "    # Set Index\n",
    "    df_tangaras.set_index('ID', inplace=True)\n",
    "    \n",
    "    buffer = StringIO()\n",
    "    df_tangaras.info(buf=buffer)\n",
    "    \n",
    "    logger.debug(\"Run df_tangara_sensors:\")\n",
    "    logger.debug(f\"Data Frame Tangaras Sensors: {buffer.getvalue()}\")\n",
    "    \n",
    "    return df_tangaras"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.462646Z",
     "start_time": "2024-09-10T15:21:33.454565Z"
    }
   },
   "source": [
    "def df_data_sensors(df_tangaras: DataFrame, start_timestamp: int, end_timestamp: int, measure: str='pm25', group_by_time: str='30s') -> DataFrame:\n",
    "    \"\"\"\n",
    "    Get Measure Data Frame Sensors of all Tangara sensors that have reported data over a period of time.\n",
    "    \n",
    "    :params:\n",
    "    :df_tangaras: DataFrame, Tangaras DataFrame\n",
    "    :start_timestamp: int, timestamp datetime value, ms\n",
    "    :end_timestamp: int, timestamp datetime value, ms\n",
    "    :measure: str, choice ['pm25', 'tmp', 'hum']\n",
    "    :group_by_time: str, choice ['30s', '1m', '1h']\n",
    "\n",
    "    :return: DataFrame, Measure Data Frame Sensors\n",
    "    \"\"\"\n",
    "    # Data Frame Sensors List\n",
    "    df_sensors_list: [DataFrame] = []\n",
    "    # SQL Query Data Sensors\n",
    "    influxdb_sql_query_measure = query_measure(df_tangaras['MAC'].to_list(), start_timestamp, end_timestamp, measure, group_by_time)# print(\"influxdb_sql_query_measure:\", influxdb_sql_query_measure)\n",
    "    # print(\"influxdb_sql_query_measure:\", influxdb_sql_query_measure)\n",
    "    # InfluxDB API REST Request\n",
    "    influxdb_request = request_influxdb(influxdb_sql_query_measure)\n",
    "    # print(\"influxdb_request:\", influxdb_request)\n",
    "    # print(\"influxdb_request.text:\", influxdb_request.text)\n",
    "\n",
    "    # For each time series\n",
    "    for result in influxdb_request.json()['results']:\n",
    "        if 'series' in result:\n",
    "            # Get the series and the statement_id\n",
    "            series = result['series'][0]\n",
    "            statement_id = result.get('statement_id')\n",
    "            # print(\"series:\", series)\n",
    "            # print(\"statement_id:\", statement_id)\n",
    "\n",
    "            # DataFrame by statement_id and df_tangaras['ID']\n",
    "            columns = [\"DATETIME\", df_tangaras['ID'].to_list()[statement_id]]\n",
    "            values = series[\"values\"]\n",
    "            df_sensor = pd.DataFrame(values, columns=columns)\n",
    "            # Set Index on DATETIME\n",
    "            df_sensor.set_index('DATETIME', inplace=True)\n",
    "            # print('df_sensor.head():', df_sensor.head())\n",
    "\n",
    "            # Append to df_sensors_list\n",
    "            df_sensors_list.append(df_sensor)\n",
    "\n",
    "    # Join all df_sensors into a single DataFrame\n",
    "    df_sensors = df_sensors_list[0].join(df_sensors_list[1:]).reset_index()\n",
    "\n",
    "    # Date Time ISO 8601 Format, TZ='America/Bogota' -05:00\n",
    "    tz = timezone(timedelta(hours=-5))\n",
    "    df_sensors['DATETIME'] = df_sensors['DATETIME'].apply(lambda x: datetime.fromtimestamp(int(x) / 1000, tz=tz).isoformat())\n",
    "    df_sensors['DATETIME'] = pd.to_datetime(df_sensors['DATETIME'])\n",
    "\n",
    "    # SET GROUP BY TIME\n",
    "    GROUP_BY_TIME = {'30s': '30S', '1m': 'Min', '1h': 'H'}\n",
    "    group_by_time = GROUP_BY_TIME[group_by_time]\n",
    "\n",
    "    # Set Index\n",
    "    df_sensors.set_index('DATETIME', inplace=True)\n",
    "    df_sensors = df_sensors.asfreq(freq=group_by_time)\n",
    "    df_sensors = df_sensors.tz_convert(\"America/Bogota\")\n",
    "\n",
    "    df_sensors[df_sensors.columns.to_list()] = df_sensors[df_sensors.columns.to_list()].astype('float64')\n",
    "    \n",
    "    buffer = StringIO()\n",
    "    df_sensors.info(buf=buffer)\n",
    "    \n",
    "    logger.debug(\"Run df_data_sensors:\")\n",
    "    logger.debug(f\"Measure Data Frame Sensors: {buffer.getvalue()}\")\n",
    "    \n",
    "    return df_sensors"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:21:33.510855Z",
     "start_time": "2024-09-10T15:21:33.505718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_file_name(prefix: str, start_datetime_iso8601: str, end_datetime_iso8601: str, extension: str = '.csv') -> str:\n",
    "    # Convertir el texto en un objeto datetime\n",
    "    start_datetime = datetime.strptime(start_datetime_iso8601, '%Y-%m-%dT%H:%M:%S%z')\n",
    "    end_datetime = datetime.strptime(end_datetime_iso8601, '%Y-%m-%dT%H:%M:%S%z')\n",
    "    \n",
    "    # Formatear la fecha en un formato adecuado para archivos, incluyendo la zona horaria\n",
    "    start_datetime = start_datetime.strftime('%Y-%m-%d_%H-%M-%S_UTC%z').replace(':', '-')\n",
    "    end_datetime = end_datetime.strftime('%Y-%m-%d_%H-%M-%S_UTC%z').replace(':', '-')\n",
    "    \n",
    "    # Nombre archivo compatible\n",
    "    file_name = f\"{prefix}_{start_datetime}_{end_datetime}{extension}\"\n",
    "    \n",
    "    logger.debug(\"Run to_file_name:\")\n",
    "    logger.debug(f\"File Name: {file_name}\")\n",
    "    \n",
    "    return file_name"
   ],
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
